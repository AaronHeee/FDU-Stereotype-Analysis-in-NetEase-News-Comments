{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 脏数据清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "date = \"2017_05_11\"\n",
    "\n",
    "df_ip = pd.read_table(\"Data/user_ip.txt\")\n",
    "df_user = pd.read_table(\"Data/%s/%s_user.txt\" % (date, date))\n",
    "print(len(df_user))\n",
    "df_ip = df_ip.iloc[:, :-1]\n",
    "df_ip = df_ip.drop_duplicates([\"passport\", \"day\"])\n",
    "df_ip = df_ip[df_ip.day == int(date.replace(\"_\",\"\"))]\n",
    "df_user = df_user.iloc[:, :-2]\n",
    "del df_user[\"col_name\"]\n",
    "df = pd.merge(df_user, df_ip, how=\"left\", on = \"passport\")\n",
    "df = df.dropna()\n",
    "print(len(df))\n",
    "df.to_pickle(\"Data/%s/%s_user.p\" % (date, date))\n",
    "df.head()\n",
    "\n",
    "df_post = pd.read_table(\"Data/%s/%s_post.txt\" % (date, date))\n",
    "df_content = pd.read_table(\"Data/%s/%s_content.txt\" % (date, date))\n",
    "df_post = df_post.drop_duplicates([\"docid\"])\n",
    "df_post = df_post.dropna(subset=[\"title\"])\n",
    "df_post.iloc[:, :-2].to_pickle(\"Data/%s/%s_post.p\" % (date, date))\n",
    "df_content.iloc[:, :-1].to_pickle(\"Data/%s/%s_content.p\" % (date, date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.Sentiment import Sentiment\n",
    "from Utils.Region import Region\n",
    "from Utils.LDA import LDA\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "###  Arguments  ###\n",
    "REGION_TXT = \"Dict/region_dict/region_2018_06_18.txt\"\n",
    "\n",
    "# test\n",
    "s = Sentiment()\n",
    "r = Region(REGION_TXT)\n",
    "\n",
    "dates = [\"2017_02_24\", \"2017_05_11\", \"2017_07_01\", \"2017_10_04\", \"2017_12_05\"]\n",
    "\n",
    "def read_pickle(dates):\n",
    "    for i, date in enumerate(dates):\n",
    "        PATH_DATA = \"Data/%s\" % date\n",
    "        df_user = pd.read_pickle(os.path.join(PATH_DATA, \"%s_user.p\" % date))\n",
    "        df_user = df_user.drop_duplicates([\"passport\"])\n",
    "        df_user = r.ip_detect(df_user, on=[\"ip\"])\n",
    "\n",
    "        df = pd.read_pickle(os.path.join(PATH_DATA, \"%s_post.p\" % date))\n",
    "        df = df.drop_duplicates([\"docid\"])\n",
    "        \n",
    "        df_content = pd.read_pickle(os.path.join(PATH_DATA, \"%s_content.p\" % date))\n",
    "        df_select = pd.read_pickle(os.path.join(PATH_DATA, \"%s_select_comments.p\" % date))\n",
    "        \n",
    "        if i == 0:\n",
    "            df_ = df\n",
    "            df_user_ = df_user\n",
    "            df_content_ = df_content\n",
    "            df_select_ = df_select\n",
    "        else:\n",
    "            df_ = df_.append(df)\n",
    "            df_user_ = df_user_.append(df_user)\n",
    "            df_content_ = df_content_.append(df_content)\n",
    "            df_select_ = df_select_.append(df_select)\n",
    "            \n",
    "    df_user_ = df_user_.drop_duplicates([\"passport\"])\n",
    "    df = df.drop_duplicates([\"docid\"])\n",
    "    \n",
    "    print(len(df_))\n",
    "    print(len(df_user_))\n",
    "    print(len(df_content_))\n",
    "    print(len(df_select_))\n",
    "\n",
    "    return df_, df_user_, df_content, df_select\n",
    "        \n",
    "df, df_user, df_content, df_select = read_pickle(dates)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_province = df_user.groupby(df_user.province).count()\n",
    "df_province.head()\n",
    "df_province[df_province.index==\"北京\"][\"ip\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyecharts import Map\n",
    "df_province = df_user.groupby(df_user.province).count()\n",
    "attr = [\"黑龙江\" , \"吉林\" , \"辽宁\" , \"广东\" , \"重庆\" , \"湖北\" , \"山西\" , \"新疆\" , \"天津\" , \"上海\" , \"青海\" , \"河北\" , \"山东\" , \"广西\" , \"湖南\" , \"福建\" , \"浙江\" , \"河南\" , \"宁夏\" , \"内蒙古\" , \"西藏\" , \"台湾\" , \"江苏\" , \"香港\" , \"云南\" , \"江西\" , \"安徽\" , \"贵州\" , \"陕西\" , \"北京\" , \"甘肃\" , \"海南\" , \"澳门\" , \"四川\"]\n",
    "value = [0.028267655  , 0.00968454  , 0.074776232  , 0.541242906  , 0.068774272  , 0.091536993  , 0.061536903  , 0.077636696  , 0.057082876  , 0.432770752  , 0.018705897  , 0.055020462  , 0.143695742  , 0.102932369  , 0.103757151  , 0.103021869  , 0.118858011  , 0.192689964  , 0.002114068  , 0.012027034  , 0.016048338  , 0.18175063  , 0.227587807  , 0.30437899  , 0.178132753  , 0.054021774  , 0.089800214  , 0.043149214  , 0.091536172  , 0.338439951  , 0.028622664  , 0.025747214  , 0.087614442  , 0.159078574]\n",
    "for i, a in enumerate(attr):\n",
    "    value[i] = df_province[df_province.index==a][\"ip\"].values[0]\n",
    "map = Map(\"网易新闻用户全国分布图\", width=800, height=500)\n",
    "map.add(\"\", attr, value, maptype='china',\n",
    "        is_visualmap=True, \n",
    "        visual_text_color=\"#000\",\n",
    "        visual_range_text=[\"\", \"\"],\n",
    "        is_label_show=True,\n",
    "       visual_range=[0, max(value)])\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评论情感分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.Sentiment import Sentiment\n",
    "from Utils.Region import Region\n",
    "from Utils.LDA import LDA\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "###  Arguments  ###\n",
    "date = \"2017_12_05\"\n",
    "REGION_TXT = \"Dict/region_dict/region_2018_06_18.txt\"\n",
    "PATH_DATA = \"Data/%s\" % date\n",
    "\n",
    "# test\n",
    "\n",
    "s = Sentiment()\n",
    "r = Region(REGION_TXT)\n",
    "\n",
    "# 构造输入数据\n",
    "text = [\n",
    "    [\"潮汕人很帅，湖北人挺会做生意的！\", \"上海\"],\n",
    "    [\"老铁牛逼！\", \"重庆\"],\n",
    "    [\"我觉得很好吃啊\", \"北京\"],\n",
    "    ]\n",
    "\n",
    "df = pd.DataFrame(text, columns=[\"text\", \"src\"])\n",
    "print(df.head())\n",
    "\n",
    "df = r.region_detect(df, on=[\"text\"])\n",
    "\n",
    "# dataFrame中批量添加region字段\n",
    "print(s.sentiment_detect(df, on=[\"text\"], srcs=[\"src\"], dists=[\"region_1\", \"region_2\", \"region_3\"]))\n",
    "print(s.output_record(src = \"北京\"))\n",
    "\n",
    "\n",
    "# MySQL设定\n",
    "# engine = create_engine('mysql://root:qwert12345@localhost:3306/netease', convert_unicode=True, encoding='utf-8',\n",
    "#                        connect_args={\"charset\": \"utf8\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据读入\n",
    "path_prefix = PATH_DATA\n",
    "df = pd.read_pickle(os.path.join(path_prefix, \"%s_user.p\" % date))\n",
    "df_content = pd.read_pickle(os.path.join(path_prefix, \"%s_content.p\" % date))\n",
    "df_post = pd.read_pickle(os.path.join(path_prefix, \"%s_post.p\" % date))\n",
    "\n",
    "print(len(df))\n",
    "df = pd.merge(df, df_content, on=\"tie_id\")\n",
    "print(df.head())\n",
    "df = df.dropna(axis=1)\n",
    "df = df.dropna(axis=0)\n",
    "print(len(df))\n",
    "\n",
    "# 模型加载\n",
    "path_region_dict = REGION_TXT\n",
    "r = Region(path_region_dict)\n",
    "df = r.region_detect(df, on=[\"content\"])\n",
    "df_select = df[df[\"region_1\"] != 0]\n",
    "print(len(df_select))\n",
    "print(df_select)\n",
    "\n",
    "df_select = r.ip_detect(df_select.iloc[:], on=[\"ip\"], nbworker=8)\n",
    "print(df_select.head())\n",
    "\n",
    "# 模型存储\n",
    "df_select.to_pickle(os.path.join(path_prefix, \"%s_select_comments.p\" % date))\n",
    "# df_select.to_sql(\"%s_select_comments\" % date, engine, index=False, if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据加载\n",
    "df = pd.read_pickle(os.path.join(path_prefix, \"%s_select_comments.p\" % date))\n",
    "\n",
    "# 模型加载\n",
    "s = Sentiment()\n",
    "df = s.sentiment_detect(df, on=[\"content\"], srcs=[\"province\"], dists=[\"region_1\", \"region_2\", \"region_3\"])\n",
    "df_freq = s.table_record()\n",
    "\n",
    "# 结果保存\n",
    "df.to_pickle(os.path.join(path_prefix, \"%s_sentiment.p\" % date))\n",
    "df_freq.to_pickle(os.path.join(path_prefix, \"%s_senti_freq.p\" % date))\n",
    "print(df)\n",
    "print(df_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from Utils.Sentiment import Sentiment\n",
    "from Utils.Region import Region\n",
    "from Utils.LDA import LDA\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "###  Arguments  ###\n",
    "REGION_TXT = \"Dict/region_dict/region_2018_06_18.txt\"\n",
    "s = Sentiment()\n",
    "r = Region(REGION_TXT)\n",
    "\n",
    "###  Arguments  ###\n",
    "dates = [\"2017_02_24\", \"2017_05_11\", \"2017_07_01\", \"2017_10_04\", \"2017_12_05\"]\n",
    "# dates = [\"2017_02_24\"]\n",
    "\n",
    "def read_pickle(dates):\n",
    "    for i, date in enumerate(dates):\n",
    "        PATH_DATA = \"Data/%s\" % date\n",
    "        df_user = pd.read_pickle(os.path.join(PATH_DATA, \"%s_user.p\" % date))\n",
    "        df_user = df_user.drop_duplicates([\"passport\"])\n",
    "        df_user = r.ip_detect(df_user, on=[\"ip\"])\n",
    "\n",
    "        df = pd.read_pickle(os.path.join(PATH_DATA, \"%s_sentiment.p\" % date))\n",
    "        df = df.loc[:, [\"docid\", \"province\", \"city\", \"region_1\", \"content\", \"polar\", \"passport\"]]\n",
    "        df = df.drop_duplicates([\"content\"])\n",
    "        # 补全黑龙江和内蒙古\n",
    "        df.loc[df[\"province\"]==\"黑龙\", \"province\"] = \"黑龙江\"\n",
    "        df.loc[df[\"province\"]==\"内蒙\", \"province\"] = \"内蒙古\"\n",
    "        df_user.loc[df_user[\"province\"]==\"黑龙\", \"province\"] = \"黑龙江\"\n",
    "        df_user.loc[df_user[\"province\"]==\"内蒙\", \"province\"] = \"内蒙古\"\n",
    "        \n",
    "        df_post = pd.read_pickle(os.path.join(PATH_DATA, \"%s_post.p\" % date))\n",
    "        df_post = df_post.drop_duplicates([\"docid\"])\n",
    "        \n",
    "        df_content = pd.read_pickle(os.path.join(PATH_DATA, \"%s_content.p\" % date))\n",
    "        \n",
    "        if i == 0:\n",
    "            df_ = df\n",
    "            df_user_ = df_user\n",
    "            df_post_ = df_post\n",
    "            df_content_ = df_content\n",
    "        else:\n",
    "            df_ = df_.append(df)\n",
    "            df_user_ = df_user_.append(df_user)\n",
    "            df_post_ = df_post_.append(df_post)\n",
    "            df_content_ = df_content_.append(df_content)\n",
    "            \n",
    "        print(\"%s:\" % date)\n",
    "        print(len(df))\n",
    "        print(len(df_user.drop_duplicates([\"passport\"])))\n",
    "        print(len(df_post))\n",
    "        print(len(df_content))\n",
    "            \n",
    "    df_user_ = df_user_.drop_duplicates([\"passport\"])\n",
    "    df_post_ = df_post_.drop_duplicates([\"docid\"])  \n",
    "    \n",
    "    print(len(df_))\n",
    "    print(len(df_user_))\n",
    "    print(len(df_post_))\n",
    "    print(len(df_content_))\n",
    "\n",
    "    return df_, df_user_, df_post_, df_content_\n",
    "        \n",
    "df, df_user, df_post, df_content = read_pickle(dates)\n",
    "df_senti = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_s = df_user[df_user[\"province\"].isin(provinces_tt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = pd.merge(df_content, df_user, on = [\"tie_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = r.region_detect(df_merge, on=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(df_senti[(df_senti[\"province\"].isin(provinces_tt)) & (df_senti[\"region_1\"].isin(provinces_tt))][\"passport\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_senti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provinces_tt =  list(provinces_t[provinces_t > 500].index)\n",
    "provinces_tt.remove(\"中国\")\n",
    "len(provinces_tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 标题情感分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from Utils.Sentiment import Sentiment\n",
    "from Utils.Region import Region\n",
    "from Utils.LDA import LDA\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "###  Arguments  ###\n",
    "date = \"2017_02_24\"\n",
    "REGION_TXT = \"Dict/region_dict/region_2018_06_18.txt\"\n",
    "PATH_DATA = \"Data/%s\" % date\n",
    "\n",
    "r = Region(REGION_TXT)\n",
    "\n",
    "df_user = pd.read_pickle(os.path.join(PATH_DATA, \"%s_user.p\" % date))\n",
    "df_post = pd.read_pickle(os.path.join(PATH_DATA, \"%s_post.p\" % date))\n",
    "df_senti = pd.read_pickle(os.path.join(PATH_DATA, \"%s_sentiment.p\" % date))\n",
    "\n",
    "df_user = df_user.drop_duplicates([\"passport\"])\n",
    "df_user = r.ip_detect(df_user, on=[\"ip\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_region_dict = REGION_TXT\n",
    "r = Region(path_region_dict)\n",
    "dates = [\"2017_02_24\", \"2017_05_11\", \"2017_07_01\", \"2017_10_04\", \"2017_12_05\"]\n",
    "for date in dates:\n",
    "    if date == dates[0]:\n",
    "        df_post = pd.read_table(\"Data/%s/%s_post.txt\" % (date, date)).iloc[:, :-2]\n",
    "    else:\n",
    "        df_post = df_post.append(pd.read_table(\"Data/%s/%s_post.txt\" % (date, date)).iloc[:, :-2])\n",
    "df_post = df_post.drop_duplicates([\"docid\"])\n",
    "df_post = df_post.dropna(subset=[\"title\"])\n",
    "df_post = r.region_detect(df_post, on=[\"title\"])\n",
    "\n",
    "news = df_post\n",
    "region_news = df_post[df_post[\"region_1\"]!=0]\n",
    "select_news =  df_post[df_post[\"docid\"].isin(df_senti[df_senti[\"polar\"]<0][\"docid\"])]\n",
    "region_news_in_select = region_news[region_news[\"docid\"].isin(df_senti[df_senti[\"polar\"]<0][\"docid\"])]\n",
    "\n",
    "print(\"news:\", len(df_post), \"\\nregion_news:\", len(region_news), \"\\nselect_news:\", len(select_news), \"\\nregion_news_in_select:\", len(region_news_in_select))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_ss = df_post[df_post.region_1==\"河南\"]\n",
    "df_ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 地域印象评分矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###    省份之间评分   ###\n",
    "###   TODO:合适指标  ###\n",
    "###  这里是 差评:好评 ###\n",
    "# provinces_scores = df[df[\"polar\"]==-1].groupby([\"province\",\"region_1\"])[\"polar\"].count() / df_user.groupby(\"province\").count()[\"docid\"]\n",
    "provinces_scores = df[df[\"polar\"]==-1].groupby([\"province\",\"region_1\"])[\"polar\"].count() / df.groupby([\"province\", \"region_1\"]).count()[\"docid\"]\n",
    "# provinces_scores = df[df[\"polar\"]==-1].groupby([\"province\",\"region_1\"])[\"polar\"].count()\n",
    "provinces_scores = provinces_scores.dropna()\n",
    "provinces_t = df[df[\"polar\"]==-1].groupby([\"province\",\"region_1\"])[\"polar\"].count()\n",
    "provinces_scores = provinces_scores[provinces_t>50]\n",
    "provinces = ['黑龙江', '吉林','辽宁', '广东', '重庆', '湖北', '山西', '新疆', '天津', '上海', '青海', '河北', '山东', '广西', '湖南', '福建', '浙江', '河南', '宁夏', '内蒙古', '西藏', '台湾', '江苏', '香港', '云南', '江西', '安徽', '贵州', '陕西', '北京', '甘肃', '海南', '澳门', '四川']\n",
    "provinces_dict = {p:i for i, p in enumerate(provinces)}\n",
    "province_num = len(provinces)\n",
    "\n",
    "scores_mat = np.zeros(shape=(province_num, province_num))\n",
    "for idx in provinces_scores.index:\n",
    "    src, dist = idx\n",
    "    if src in provinces_dict and dist in provinces:\n",
    "        scores_mat[provinces_dict[src], provinces_dict[dist]] = provinces_scores[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provinces1 = df[df[\"polar\"]==-1].groupby([\"province\"])[\"polar\"].count()\n",
    "provinces2 = df[df[\"polar\"]==-1].groupby([\"region_1\"])[\"polar\"].count()\n",
    "provinces =  df[df[\"polar\"]==-1].groupby([\"region_1\"])[\"polar\"].count()\n",
    "for i in provinces1.index:\n",
    "    if i not in provinces:\n",
    "        del provinces1[i]\n",
    "provinces1[(provinces2>500) & (provinces1>500)]\n",
    "provinces2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table(\"Results/他人负评比筛选.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:, [1,2]].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###    省份之间评分   ###\n",
    "###   TODO:合适指标  ###\n",
    "###  这里是 差评:好评 ###\n",
    "# provinces_scores = df[df[\"polar\"]==-1].groupby([\"province\",\"region_1\"])[\"polar\"].count() / df_user.groupby(\"province\").count()[\"docid\"]\n",
    "provinces_scores = df[(df[\"polar\"]==-1)&(df[\"region_1\"]!=df[\"province\"])].groupby([\"region_1\"])[\"polar\"].count() / df.groupby([\"region_1\"]).count()[\"docid\"]\n",
    "provinces_ego = df[(df[\"polar\"]==-1)&(df[\"region_1\"]==df[\"province\"])].groupby([\"region_1\"])[\"polar\"].count() / df.groupby([\"region_1\"]).count()[\"docid\"]\n",
    "provinces_out = df[(df[\"polar\"]==-1)&(df[\"region_1\"]!=df[\"province\"])].groupby([\"province\"])[\"polar\"].count() / df.groupby([\"province\"]).count()[\"docid\"]\n",
    "df_score = pd.DataFrame(data=[provinces_scores, provinces_ego])\n",
    "# provinces_scores = df[df[\"polar\"]==-1].groupby([\"province\",\"region_1\"])[\"polar\"].count()\n",
    "provinces_scores = provinces_scores.dropna()\n",
    "for i in provinces_out.index:\n",
    "    if i not in provinces:\n",
    "        del provinces_out[i]\n",
    "provinces_out = provinces_out.dropna()\n",
    "provinces_t = df[df[\"polar\"]==-1].groupby([\"province\",\"region_1\"])[\"polar\"].count()\n",
    "provinces = ['黑龙江', '吉林','辽宁', '广东', '重庆', '湖北', '山西', '新疆', '天津', '上海', '青海', '河北', '山东', '广西', '湖南', '福建', '浙江', '河南', '宁夏', '内蒙古', '西藏', '台湾', '江苏', '香港', '云南', '江西', '安徽', '贵州', '陕西', '北京', '甘肃', '海南', '澳门', '四川']\n",
    "\n",
    "provinces_scores.to_csv(\"Results/他人遭受负评比.txt\", sep= \"\\t\")\n",
    "provinces_ego.to_csv(\"Results/自我负评比.txt\", sep = \"\\t\")\n",
    "provinces_out.to_csv(\"Results/他人发出负评比.txt\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 相关性分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = []\n",
    "provinces_scores\n",
    "for p1 in provinces:\n",
    "    for p2 in provinces:\n",
    "        if p1!=p2 and (p1, p2) in provinces_t and (p2, p1) in provinces_t and provinces_t[p1, p2]>100 and provinces_t[p2, p1]>100:\n",
    "            s.append([provinces_scores[p1, p2], provinces_scores[p2, p1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provinces_t[provinces_t > 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(data = s)\n",
    "test.plot(alpha=0.7)\n",
    "test.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import *  \n",
    "\n",
    "# title = \"%s负面评价绝对数量矩阵\" % date\n",
    "\n",
    "title = \"建构周负面评价比例矩阵\"\n",
    "platform = \"mac\"\n",
    "if platform == \"mac\":\n",
    "    mpl.rcParams['font.sans-serif'] = ['STHeiti']  \n",
    "elif platform == \"win\":\n",
    "    mpl.rcParams['font.sans-serif'] = ['FangSong']  \n",
    "\n",
    "res = {'score':scores_mat, 'area':provinces}\n",
    "score = np.nan_to_num(res['score'])\n",
    "# score[score<=0.5] = 0\n",
    "area = res['area']\n",
    "\n",
    "# Plot it out\n",
    "fig, ax = plt.subplots()\n",
    "heatmap = ax.pcolor(score, cmap=plt.cm.Blues)\n",
    "\n",
    "# Format\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(17, 15)\n",
    "\n",
    "# turn off the frame\n",
    "ax.set_frame_on(False)\n",
    "\n",
    "# put the major ticks at the middle of each cell\n",
    "ax.set_yticks(np.arange(len(area)) + 0.5, minor=False)\n",
    "ax.set_xticks(np.arange(len(area)) + 0.5, minor=False)\n",
    "\n",
    "# want a more natural, table-like display\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Set the labels\n",
    "\n",
    "# label source:https://en.wikipedia.org/wiki/Basketball_statistics\n",
    "labels = area\n",
    "\n",
    "# note I could have used nba_sort.columns but made \"labels\" instead\n",
    "ax.set_xticklabels(labels, minor=False)\n",
    "ax.set_yticklabels(labels, minor=False)\n",
    "\n",
    "# rotate the\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "ax.grid(False)\n",
    "\n",
    "# Turn off all the ticks\n",
    "ax = plt.gca()\n",
    "\n",
    "for t in ax.xaxis.get_major_ticks():\n",
    "    t.tick1On = False\n",
    "    t.tick2On = False\n",
    "for t in ax.yaxis.get_major_ticks():\n",
    "    t.tick1On = False\n",
    "    t.tick2On = False\n",
    "    \n",
    "# cbar = plt.colorbar(heatmap, shrink=0.5, ticks=np.linspace(0,100,11)) \n",
    "# cbar.set_ticklabels(np.linspace(0,100,11))\n",
    "cbar = plt.colorbar(heatmap, shrink=0.5)\n",
    "\n",
    "plt.title(title)\n",
    "plt.savefig(\"%s.jpg\" % title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_mat_ = np.zeros(shape=(province_num+1, province_num+1))\n",
    "scores_mat_[1:, 1:] = scores_mat\n",
    "pd.DataFrame(data=scores_mat, columns=provinces, index=provinces).to_csv(\"%s.csv\" % title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 印象标签\n",
    "## LDA模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from Utils.LDA import LDA\n",
    "src = \"河南\"\n",
    "\n",
    "for polar in [1]:\n",
    "    for tar in provinces:\n",
    "\n",
    "        # comments = list(df [(df.polar==polar) & (df.province == src) & (df.region_1 == tar)][\"content\"].values)\n",
    "        comments = list(df[(df.region_1 == tar)][\"content\"].values)\n",
    "        # comments = list(df[df.region_1 == tar][\"content\"].values)\n",
    "        with open(\"主题生成.txt\", \"a\") as f:\n",
    "            l = LDA()\n",
    "            labels = l.label_detect(comments, num_topics=1, num_words=50)[0]\n",
    "            \n",
    "            print(\"\\n%s\" % tar, end=\"\\n======\\n\", file=f)\n",
    "            for site, num in labels:\n",
    "                print(\"%s\\t%s\" % (site, int(num*1000)), file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查询相关信息\n",
    "## 印象查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s[(df_s.polar == polar)].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src = \"湖北\"\n",
    "tar = \"河南\"\n",
    "polar = -1\n",
    "\n",
    "# df_s = df[df.docid == \"CK5303000007871O\"]\n",
    "\n",
    "# 迪士尼争端：CK5303000007871O\n",
    "# 高考：CE02F79K000181BT\n",
    "# 上海人挑事：CE1QKQ3G0518CSOA\n",
    "# 上海人挑事被群嘲：CK62NS3S00018AOQ\n",
    "# 香港对上海有成见，指责上海市民小气自私：CVR1AEA90524DEGV\n",
    "\n",
    "df_s = df[(df.polar==polar) & (df.region_1 == tar)]\n",
    "# df_s = df[(df.province != df.region_1) & (df.polar==polar) & (df.region_1 == tar)]\n",
    "# df_s = df[(df.province != df.region_1) & (df.region_1 == tar)]\n",
    "\n",
    "# df_s = df_s[df_s.polar == polar].groupby([\"region_1\"]).count() /  df_s.groupby([\"region_1\"]).count()\n",
    "\n",
    "df_s.to_csv(\"%s.txt\" % (tar), sep=\"\\t\")\n",
    "# df_s[\"docid\"].to_csv(\"Results/重庆攻击范围.txt\", sep=\"\\t\")\n",
    "df_s.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user[df_user.province==\"台湾\"].count()/df_user.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.LDA import LDA\n",
    "\n",
    "comments = list(df_s[\"content\"].values)\n",
    "\n",
    "l = LDA()\n",
    "labels = l.label_detect(comments, num_topics=1, num_words=100)[0]\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标题查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [\"2017_02_24\", \"2017_05_11\", \"2017_07_01\", \"2017_10_04\", \"2017_12_05\"]\n",
    "date = dates[3]\n",
    "PATH_DATA = \"Data/%s\" % date\n",
    "df_post = pd.read_pickle(os.path.join(PATH_DATA, \"%s_post.p\" % date))\n",
    "df_post[df_post[\"docid\"] == \"CVR1AEA90524DEGV\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用户查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 人工评测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = \"2017_02_24\"\n",
    "\n",
    "PATH_DATA = \"Data/%s\" % date\n",
    "path_prefix = PATH_DATA\n",
    "df = pd.read_pickle(os.path.join(path_prefix, \"%s_user.p\" % date))\n",
    "df_content = pd.read_pickle(os.path.join(path_prefix, \"%s_content.p\" % date))\n",
    "\n",
    "df = pd.merge(df, df_content, on=\"tie_id\")\n",
    "df = df.dropna(axis=1)\n",
    "df = df.dropna(axis=0)\n",
    "df_sample = df.sample(n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Region(path_region_dict)\n",
    "s = Sentiment()\n",
    "df_sample = r.region_detect(df_sample, on=[\"content\"])\n",
    "df_sample = s.sentiment_detect(df_sample, on=[\"content\"], srcs=[\"content\"], dists=[\"region_1\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
